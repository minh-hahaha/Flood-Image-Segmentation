{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a09dc6a",
   "metadata": {
    "id": "3a09dc6a"
   },
   "source": [
    "\n",
    "# Flood Segmentation — FloodNet **Supervised v1.0** (Working Config)\n",
    "\n",
    "This version is hard-wired for the real FloodNet layout:\n",
    "\n",
    "```\n",
    ".../FloodNet-Supervised_v1.0/\n",
    "  train/{train-org-img, train-label-img}\n",
    "  val/{val-org-img, val-label-img}\n",
    "  test/{test-org-img, test-label-img}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for multiprocessing issues in Jupyter notebooks\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "# Also set num_workers to 0 to avoid multiprocessing issues\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107404b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "6107404b",
    "outputId": "5308ab0d-2d4a-43de-efe3-8ef93ee7ca50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Data input folders\n",
      "Train IMG: /Users/minhha/Downloads/FloodNet/FloodNet-Supervised_v1.0/train/train-org-img   Good\n",
      "Train MSK: /Users/minhha/Downloads/FloodNet/FloodNet-Supervised_v1.0/train/train-label-img   Good\n",
      "Val   IMG: /Users/minhha/Downloads/FloodNet/FloodNet-Supervised_v1.0/val/val-org-img   Good\n",
      "Val   MSK: /Users/minhha/Downloads/FloodNet/FloodNet-Supervised_v1.0/val/val-label-img   Good\n",
      "Test  IMG: /Users/minhha/Downloads/FloodNet/FloodNet-Supervised_v1.0/test/test-org-img   Good\n",
      "Test  MSK: /Users/minhha/Downloads/FloodNet/FloodNet-Supervised_v1.0/test/test-label-img   Good\n",
      "Sample train images: []\n",
      "Sample train masks:  ['10165_lab.png', '10166_lab.png', '10168_lab.png', '10170_lab.png', '10171_lab.png']\n"
     ]
    }
   ],
   "source": [
    "# If you're on Colab, uncomment:\n",
    "!pip -q install torch torchvision torchaudio opencv-python scikit-learn matplotlib\n",
    "\n",
    "\n",
    "import os, random, time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # ROOT_DIR: str = \"/content/drive/MyDrive/FloodNet/FloodNet-Supervised_v1.0\"\n",
    "    # PALETTE_XLSX: str = \"/content/drive/MyDrive/FloodNet/ColorMasks-FloodNetv1.0/ColorPalette-Values.xlsx\"\n",
    "    ROOT_DIR: str = \"/Users/minhha/Downloads/FloodNet/FloodNet-Supervised_v1.0\"\n",
    "    PALETTE_XLSX: str = \"/Users/minhha/Downloads/FloodNet/ColorMasks-FloodNetv1.0/ColorPalette-Values.xlsx\"\n",
    "\n",
    "    # Optional: palette excel (class names + colors). You can keep using the one from the ColorMasks download:\n",
    "\n",
    "    NUM_CLASSES: int = 10\n",
    "    INPUT_SIZE: int = 512\n",
    "    BATCH_SIZE: int = 4\n",
    "    EPOCHS: int = 15\n",
    "    LR: float = 1e-4\n",
    "    NUM_WORKERS: int = 0 # Set to 0 to avoid multiprocessing issues in Jupyter\n",
    "    USE_PRETRAINED: bool = True\n",
    "    OUT_DIR: str = \"./outputs_floodnet_supervised\"\n",
    "\n",
    "cfg = CFG()\n",
    "os.makedirs(cfg.OUT_DIR, exist_ok=True)\n",
    "# SAFE MODE: avoid multiprocessing issues in Jupyter\n",
    "cfg.NUM_WORKERS = 0   # <- important for Jupyter notebooks\n",
    "PIN_MEMORY = False    # we'll pass this into DataLoaders\n",
    "\n",
    "# (optional) reduce batch size temporarily to speed up first batch\n",
    "cfg.BATCH_SIZE = 2\n",
    "\n",
    "# (optional) reduce epochs for initial check\n",
    "cfg.EPOCHS = 5\n",
    "\n",
    "# (optional) avoid OpenCV thread contention\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Resolve real FloodNet paths\n",
    "TRAIN_IMG_DIR = str(Path(cfg.ROOT_DIR) / \"train\" / \"train-org-img\")\n",
    "TRAIN_MSK_DIR = str(Path(cfg.ROOT_DIR) / \"train\" / \"train-label-img\")\n",
    "VAL_IMG_DIR   = str(Path(cfg.ROOT_DIR) / \"val\" / \"val-org-img\")\n",
    "VAL_MSK_DIR   = str(Path(cfg.ROOT_DIR) / \"val\" / \"val-label-img\")\n",
    "TEST_IMG_DIR  = str(Path(cfg.ROOT_DIR) / \"test\" / \"test-org-img\")\n",
    "TEST_MSK_DIR  = str(Path(cfg.ROOT_DIR) / \"test\" / \"test-label-img\")\n",
    "\n",
    "print(\"Step: Data input folders\")\n",
    "for t, p in [\n",
    "    (\"Train IMG\", TRAIN_IMG_DIR), (\"Train MSK\", TRAIN_MSK_DIR),\n",
    "    (\"Val   IMG\", VAL_IMG_DIR),   (\"Val   MSK\", VAL_MSK_DIR),\n",
    "    (\"Test  IMG\", TEST_IMG_DIR),  (\"Test  MSK\", TEST_MSK_DIR),\n",
    "]:\n",
    "    print(f\"{t}: {p}  \", \"Good\" if Path(p).exists() else \"Bad\")\n",
    "\n",
    "# Quick peek at some files (non-fatal if empty)\n",
    "def _peek(globpat, n=5):\n",
    "    xs = sorted(glob(globpat))[:n]\n",
    "    return [Path(x).name for x in xs]\n",
    "\n",
    "print(\"Sample train images:\", _peek(str(Path(TRAIN_IMG_DIR) / \"*.png\")))\n",
    "print(\"Sample train masks: \", _peek(str(Path(TRAIN_MSK_DIR) / \"*_lab.png\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e059870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the dataset class is properly defined for multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    # This ensures the dataset class is available when using multiprocessing\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5989cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a5989cc",
    "outputId": "7c9d4083-0f98-4da3-88b8-4ddb90f620e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Step: Loaded palette\n",
      "   0: background             (0, 0, 0)\n",
      "   1: building-flooded       (255, 0, 0)\n",
      "   2: building-non-flooded   (180, 120, 120)\n",
      "   3: road-flooded           (160, 150, 20)\n",
      "   4: road-non-flooded       (140, 140, 140)\n",
      "   5: water                  (61, 230, 250)\n",
      "   6: tree                   (0, 82, 255)\n",
      "   7: vehicle                (255, 0, 245)\n",
      "   8: pool                   (255, 235, 0)\n",
      "   9: grass                  (4, 250, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_palette_from_excel(xlsx_path):\n",
    "    if not Path(xlsx_path).exists():\n",
    "        names = ['background','building-flooded','building-non-flooded','road-flooded','road-non-flooded','water','tree','vehicle','pool','grass']\n",
    "        colors = [(0,0,0),(255,0,0),(180,120,120),(160,150,20),(140,140,140),(61,230,250),(0,82,255),(255,0,245),(255,235,0),(4,250,7)]\n",
    "        return {tuple(c):i for i,c in enumerate(colors)}, names, colors\n",
    "    df = pd.read_excel(xlsx_path, header=None)\n",
    "    rows = df[[6,7]].dropna()\n",
    "    names, colors = [], []\n",
    "    for _, row in rows.iterrows():\n",
    "        name = str(row[6]).strip()\n",
    "        rgb = row[7]\n",
    "        if isinstance(rgb, str):\n",
    "            rgb = tuple(int(x.strip()) for x in rgb.strip(\"() \").split(\",\"))\n",
    "        else:\n",
    "            rgb = tuple(int(x) for x in rgb)\n",
    "        names.append(name); colors.append(rgb)\n",
    "    return {tuple(c):i for i,c in enumerate(colors)}, names, colors\n",
    "\n",
    "PALETTE_MAP, CLASS_NAMES, CLASS_COLORS = load_palette_from_excel(cfg.PALETTE_XLSX)\n",
    "print(\"▶️ Step: Loaded palette\")\n",
    "for i,(n,c) in enumerate(zip(CLASS_NAMES, CLASS_COLORS)):\n",
    "    print(f\"  {i:2d}: {n:22s} {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d159f9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d159f9a",
    "outputId": "a9c9abb0-252e-4140-c670-0f6cb0b2fece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Building dataloaders...\n",
      "Found 1445 pairs in train. Missing matches for 0 masks.\n",
      "Found 450 pairs in val. Missing matches for 0 masks.\n",
      "Data ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMAGE_SUFFIXES = (\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\")\n",
    "IMAGE_GUESS_PATTERNS = [\"{id}.png\",\"{id}.jpg\",\"{id}_img.png\",\"{id}_pre.png\",\"{id}_sat.png\",\"{id}_rgb.png\"]\n",
    "\n",
    "def basename_id(p: str):\n",
    "    name = Path(p).name\n",
    "    if \"_lab\" in name:\n",
    "        return name.split(\"_lab\")[0]\n",
    "    return Path(p).stem\n",
    "\n",
    "def find_rgb_for_id(image_dir: str, base: str):\n",
    "    for pat in IMAGE_GUESS_PATTERNS:\n",
    "        cand = Path(image_dir)/pat.format(id=base)\n",
    "        if cand.exists():\n",
    "            return str(cand)\n",
    "    for ext in IMAGE_SUFFIXES:\n",
    "        c = Path(image_dir)/(base+ext)\n",
    "        if c.exists():\n",
    "            return str(c)\n",
    "    hits = []\n",
    "    for ext in IMAGE_SUFFIXES:\n",
    "        hits += glob(str(Path(image_dir)/f\"{base}*{ext}\"))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def rgb_mask_to_index(mask_bgr: np.ndarray):\n",
    "    mask_rgb = mask_bgr[:,:,::-1]\n",
    "    out = np.full(mask_rgb.shape[:2], 255, np.uint8)  # 255 ignore\n",
    "    for (r,g,b), idx in PALETTE_MAP.items():\n",
    "        m = (mask_rgb[:,:,0]==r) & (mask_rgb[:,:,1]==g) & (mask_rgb[:,:,2]==b)\n",
    "        out[m] = idx\n",
    "    return out\n",
    "\n",
    "class FloodNetDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, split, size=512, augment=False):\n",
    "        self.img_dir, self.mask_dir, self.split = img_dir, mask_dir, split\n",
    "        self.size, self.augment = size, augment\n",
    "        self.mask_paths = sorted(glob(str(Path(mask_dir) / \"*.png\")))\n",
    "        self.pairs = []\n",
    "        misses = 0\n",
    "        for mp in self.mask_paths:\n",
    "            base = basename_id(mp)\n",
    "            ip = find_rgb_for_id(img_dir, base)\n",
    "            if ip is None:\n",
    "                misses += 1\n",
    "            else:\n",
    "                self.pairs.append((ip, mp))\n",
    "        print(f\"Found {len(self.pairs)} pairs in {split}. Missing matches for {misses} masks.\")\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def _rand_resize(self, img, mask, smin=0.9, smax=1.1):\n",
    "        import cv2, numpy as np, random\n",
    "        h,w = mask.shape\n",
    "        s = random.uniform(smin, smax)\n",
    "        nh, nw = int(h*s), int(w*s)\n",
    "        img2  = cv2.resize(img,  (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "        mask2 = cv2.resize(mask, (nw, nh), interpolation=cv2.INTER_NEAREST)\n",
    "        # center crop/pad back to (h,w)\n",
    "        top  = max((nh-h)//2, 0); left = max((nw-w)//2, 0)\n",
    "        img2  = img2[top:top+h, left:left+w]\n",
    "        mask2 = mask2[top:top+h, left:left+w]\n",
    "        if img2.shape[0]!=h or img2.shape[1]!=w:  # pad if needed\n",
    "            pad_h, pad_w = h-img2.shape[0], w-img2.shape[1]\n",
    "            img2  = cv2.copyMakeBorder(img2,0,pad_h,0,pad_w,cv2.BORDER_REFLECT)\n",
    "            mask2 = cv2.copyMakeBorder(mask2,0,pad_h,0,pad_w,cv2.BORDER_REFLECT)\n",
    "        return img2, mask2\n",
    "\n",
    "    def _aug(self, img, mask):\n",
    "        if random.random() < 0.5:\n",
    "            img = np.fliplr(img).copy()\n",
    "            mask = np.fliplr(mask).copy()\n",
    "        if random.random() < 0.5:\n",
    "            img = np.flipud(img).copy()\n",
    "            mask = np.flipud(mask).copy()\n",
    "        if random.random()<0.7:\n",
    "            img, mask = self._rand_resize(img, mask, 0.95, 1.07)\n",
    "        return img, mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "\n",
    "        # Read RGB input normally\n",
    "        img = cv2.imread(ip, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(ip)\n",
    "\n",
    "        # Read mask as a single-channel class index map\n",
    "        mask = cv2.imread(mp, cv2.IMREAD_UNCHANGED)\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(mp)\n",
    "\n",
    "        # Resize\n",
    "        img  = cv2.resize(img,  (cfg.INPUT_SIZE, cfg.INPUT_SIZE), interpolation=cv2.INTER_LINEAR).copy()\n",
    "        mask = cv2.resize(mask, (cfg.INPUT_SIZE, cfg.INPUT_SIZE), interpolation=cv2.INTER_NEAREST).copy()\n",
    "\n",
    "        # Augment\n",
    "        if self.augment and self.split == \"train\":\n",
    "            img, mask = self._aug(img, mask)\n",
    "\n",
    "        # Convert to tensor (BGR → RGB)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        chw = np.ascontiguousarray(img_rgb.transpose(2, 0, 1))\n",
    "        img_t = torch.from_numpy(chw).float() / 255.0\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "        img_t = (img_t - mean) / std\n",
    "\n",
    "        mask_t = torch.from_numpy(mask.astype(np.int64))\n",
    "        return img_t, mask_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_loader(img_dir, mask_dir, split, augment, shuffle):\n",
    "    ds = FloodNetDataset(img_dir, mask_dir, split, size=cfg.INPUT_SIZE, augment=augment)\n",
    "    if split == \"train\":\n",
    "        # Build per-sample weights favoring rare classes (presence-based)\n",
    "        weights_by_class = np.array([\n",
    "            0.7853, 0.8961, 0.6962, 0.7499, 0.4871,\n",
    "            0.3552, 0.2736, 2.8701, 2.7268, 0.1598\n",
    "        ], dtype=np.float32)\n",
    "        sample_weights = []\n",
    "        for _, mpath in ds.pairs:\n",
    "            mask = cv2.imread(mpath, cv2.IMREAD_UNCHANGED)\n",
    "            if mask is None:\n",
    "                present_weight = 1.0\n",
    "            else:\n",
    "                vals = np.unique(mask[mask!=255])\n",
    "                present_weight = float(weights_by_class[vals].sum()) if len(vals)>0 else 1.0\n",
    "                present_weight = min(present_weight, 1.4)\n",
    "            sample_weights.append(present_weight)\n",
    "        from torch.utils.data import WeightedRandomSampler\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        return ds, DataLoader(\n",
    "            ds,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            sampler=sampler,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "            persistent_workers=False,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    else:\n",
    "        return ds, DataLoader(\n",
    "            ds,\n",
    "            batch_size=cfg.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.NUM_WORKERS,\n",
    "            pin_memory=False,\n",
    "            persistent_workers=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Step: Building dataloaders...\")\n",
    "train_ds, train_dl = make_loader(TRAIN_IMG_DIR, TRAIN_MSK_DIR, \"train\", True, True)\n",
    "val_ds,   val_dl   = make_loader(VAL_IMG_DIR,   VAL_MSK_DIR,   \"val\",   False, False)\n",
    "print(\"Data ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b936cb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b936cb5",
    "outputId": "caacceb3-bf10-4667-a4ef-19f727cb0a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Step: Building model (DeepLabV3-ResNet50)\n",
      "Model ready on cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"▶️ Step: Building model (DeepLabV3-ResNet50)\")\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "try:\n",
    "    from torchvision.models.segmentation import DeepLabV3_ResNet50_Weights\n",
    "    weights = DeepLabV3_ResNet50_Weights.DEFAULT if cfg.USE_PRETRAINED else None\n",
    "    model = deeplabv3_resnet50(weights=weights)\n",
    "except Exception as e:\n",
    "    print(\"  Could not use weights API:\", e)\n",
    "    model = deeplabv3_resnet50(weights=None)\n",
    "\n",
    "try:\n",
    "    in_ch = model.classifier[-1].in_channels\n",
    "    model.classifier[-1] = nn.Conv2d(in_ch, cfg.NUM_CLASSES, 1)\n",
    "except Exception:\n",
    "    in_ch = model.classifier[4].in_channels\n",
    "    model.classifier[4] = nn.Conv2d(in_ch, cfg.NUM_CLASSES, 1)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.LR)\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "steps_per_epoch = len(train_dl)   # requires train_dl already defined\n",
    "scheduler = OneCycleLR(\n",
    "    opt,\n",
    "    max_lr=cfg.LR,\n",
    "    epochs=cfg.EPOCHS,            # or 15 if you want fixed\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=10.0,\n",
    "    final_div_factor=10.0\n",
    ")\n",
    "\n",
    "\n",
    "# --- Loss: class-weighted CrossEntropy + Dice ---\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class weights derived from your validation supports (sqrt-inv-freq, normalized)\n",
    "_class_weights = torch.tensor([\n",
    "    0.7853, 0.8961, 0.6962, 0.7499, 0.4871,\n",
    "    0.3552, 0.2736, 2.8701, 2.7268, 0.1598\n",
    "], dtype=torch.float32, device=device)\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, num_classes, ignore_index=255, ce_weight=0.7, dice_weight=0.3):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce = nn.CrossEntropyLoss(weight=_class_weights, ignore_index=ignore_index)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # CE part\n",
    "        loss = self.ce(logits, targets)\n",
    "\n",
    "        # Dice part (per-class), masked by ignore_index\n",
    "        with torch.no_grad():\n",
    "            valid = (targets != self.ignore_index).float().unsqueeze(1)\n",
    "        probs = F.softmax(logits, dim=1) * valid\n",
    "        onehot = F.one_hot(targets.clamp(min=0), num_classes=logits.shape[1]).permute(0,3,1,2).float() * valid\n",
    "        intersect = (probs * onehot).sum(dim=(0,2,3))\n",
    "        union = probs.sum(dim=(0,2,3)) + onehot.sum(dim=(0,2,3))\n",
    "        dice = 1.0 - (2.0*intersect + 1e-6) / (union + 1e-6)\n",
    "        dice_loss = dice.mean()\n",
    "\n",
    "        return self.ce_weight*loss + self.dice_weight*dice_loss\n",
    "\n",
    "# right above ComboLoss\n",
    "weights = torch.tensor([\n",
    "    0.7853, 0.8961, 0.6962, 0.7499, 0.4871,\n",
    "    0.3552, 0.2736, 2.8701, 2.7268, 0.1598\n",
    "], dtype=torch.float32, device=device)\n",
    "weights[7] *= 0.7    # vehicles\n",
    "weights[8] *= 0.8    # pools\n",
    "_class_weights = weights\n",
    "crit = ComboLoss(num_classes=cfg.NUM_CLASSES, ignore_index=255, ce_weight=0.85, dice_weight=0.15)\n",
    "print(\"Model ready on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26e361",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c26e361",
    "outputId": "c6aa9bab-0c16-447c-c4ce-d1663e687231"
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "print(\"Unique labels in batch:\", torch.unique(yb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949e36b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405,
     "referenced_widgets": [
      "37a1839c55ff423194b60477b09c840e",
      "61d2c028babd42c79abb76609a091533",
      "a45e090d017c4c4590b11b824600948a",
      "ccb5cedd93cc4a0aad6030d6d55cba71",
      "fa0daaf4810f4e61a3f769d0da747433",
      "f3943ed8c5a44048ac05d27631ab0f02",
      "41dccc7e7e7342aa9e2f074b204cc065",
      "7773863bf8f246198d4ee337bd97007f",
      "3aa93ffc39f64d228ec7d2560f6e07f1",
      "8a0cb55488d742a196e0c11c128e0676",
      "1d8b6b48b5004c1e954adfac40e360ad"
     ]
    },
    "id": "4949e36b",
    "outputId": "7df06e38-9a44-45d2-f6f9-dd0d2ed4bf1d"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "def train_one_epoch(epoch):\n",
    "    model.train(); total=0.0\n",
    "    for x,y in tqdm(train_dl, desc=f\"Epoch {epoch} • Training\", leave=False):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        out = model(x)['out']\n",
    "        out_dict = model(x)\n",
    "        main_logits = out_dict['out']\n",
    "        aux_logits  = out_dict.get('aux', None)\n",
    "\n",
    "        loss = crit(main_logits, y)\n",
    "        if aux_logits is not None:\n",
    "            loss = loss + 0.2 * nn.CrossEntropyLoss(ignore_index=255)(aux_logits, y)\n",
    "        loss.backward(); opt.step(); scheduler.step()\n",
    "        total += loss.item()*x.size(0)\n",
    "    return total/len(train_ds)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval(); total=0.0; accs=[]\n",
    "    for x,y in val_dl:\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        o = model(x)['out']; loss = crit(o,y)\n",
    "        total += loss.item()*x.size(0)\n",
    "        accs.append((o.argmax(1)==y).float().mean().item())\n",
    "    return total/len(val_ds), float(np.mean(accs)) if accs else 0.0\n",
    "\n",
    "print(\"▶️ Step: Train/Eval...\")\n",
    "best=1e9\n",
    "for e in range(1, cfg.EPOCHS+1):\n",
    "    tl = train_one_epoch(e)\n",
    "    vl, va = evaluate()\n",
    "    print(f\"Epoch {e:02d}  train_loss={tl:.4f}  val_loss={vl:.4f}  val_acc={va:.4f}\")\n",
    "    if vl<best:\n",
    "        best=vl\n",
    "        torch.save(model.state_dict(), Path(cfg.OUT_DIR)/\"best_model.pt\")\n",
    "        print(\"Saved best model\")\n",
    "print(\"Training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813a895",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c813a895",
    "outputId": "9af6f5e3-ad3c-4286-8d9e-ea976a4d86b2"
   },
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "sample_mask = sorted(glob(str(Path(TRAIN_MSK_DIR) / \"*_lab.png\")))[0]\n",
    "mask = cv2.imread(sample_mask, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "print(\"Shape:\", mask.shape, \"dtype:\", mask.dtype, \"unique values:\", np.unique(mask)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c80fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "ed8c80fe",
    "outputId": "7c6d331a-7905-4f0b-ca69-cbdc60665f29"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics_report(loader, split=\"val\"):\n",
    "    model.eval()\n",
    "    preds_all, gts_all = [], []\n",
    "    for x,y in loader:\n",
    "        x = x.to(device)\n",
    "        p = model(x)['out'].argmax(1).cpu().numpy().reshape(-1)\n",
    "        g = y.numpy().reshape(-1)\n",
    "        m = g!=255\n",
    "        preds_all.append(p[m]); gts_all.append(g[m])\n",
    "    if not preds_all:\n",
    "        print(\"No batches to score.\"); return\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    gts_all = np.concatenate(gts_all)\n",
    "\n",
    "    labels = list(range(cfg.NUM_CLASSES))\n",
    "    cm = confusion_matrix(gts_all, preds_all, labels=labels)\n",
    "    rep = classification_report(\n",
    "        gts_all, preds_all,\n",
    "        labels=labels,\n",
    "        target_names=[str(i) for i in labels],\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(rep)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_tta(x):\n",
    "    ys = []\n",
    "    for flip in [(False,False),(True,False),(False,True),(True,True)]:\n",
    "        xf = torch.flip(x, dims=[2] if flip[0] else [])  # H\n",
    "        xf = torch.flip(xf, dims=[3] if flip[1] else []) # W\n",
    "        logits = model(xf)['out']\n",
    "        # unflip logits back\n",
    "        if flip[0]: logits = torch.flip(logits, dims=[2])\n",
    "        if flip[1]: logits = torch.flip(logits, dims=[3])\n",
    "        ys.append(logits)\n",
    "    return torch.stack(ys).mean(0)  # logit-avg\n",
    "\n",
    "\n",
    "print(\"Step: Metrics on val\")\n",
    "metrics_report(val_dl, \"val\")\n",
    "print(\"Metrics done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709749a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "709749a9",
    "outputId": "bde8d4d4-bad9-4fbc-cb25-119952765240"
   },
   "outputs": [],
   "source": [
    "print(\"▶️ Step: mIoU (torchmetrics, GPU)\")\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# (Optional) always eval the best checkpoint\n",
    "best_ckpt = Path(cfg.OUT_DIR) / \"best_model.pt\"\n",
    "if best_ckpt.exists():\n",
    "    model.load_state_dict(torch.load(best_ckpt, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "try:\n",
    "    from torchmetrics.classification import MulticlassJaccardIndex\n",
    "except Exception:\n",
    "    from torchmetrics import JaccardIndex as MulticlassJaccardIndex  # older torchmetrics\n",
    "\n",
    "# ---- Overall mIoU on GPU ----\n",
    "miou = MulticlassJaccardIndex(num_classes=cfg.NUM_CLASSES, ignore_index=255).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_miou_gpu(loader):\n",
    "    miou.reset()\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        preds = model(x)['out'].argmax(1)\n",
    "        logits = predict_tta(x)\n",
    "        preds = logits.argmax(1)\n",
    "        miou.update(preds, y)\n",
    "    val = miou.compute()\n",
    "    val = val.item() if hasattr(val, \"item\") else float(val)\n",
    "    print(f\"mIoU (GPU): {val:.4f}\")\n",
    "\n",
    "compute_miou_gpu(val_dl)\n",
    "print(\"mIoU done (GPU)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g5XKgAtuRy-F",
   "metadata": {
    "id": "g5XKgAtuRy-F"
   },
   "outputs": [],
   "source": [
    "print(\"▶️ Step: mIoU (torchmetrics)\")\n",
    "try:\n",
    "    from torchmetrics.classification import MulticlassJaccardIndex\n",
    "except Exception:\n",
    "    from torchmetrics import JaccardIndex as MulticlassJaccardIndex\n",
    "\n",
    "miou_pc = MulticlassJaccardIndex(num_classes=cfg.NUM_CLASSES, ignore_index=255, average=None).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def per_class_iou_gpu(loader):\n",
    "    miou_pc.reset()\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        preds = model(x)['out'].argmax(1)\n",
    "        logits = predict_tta(x)\n",
    "        preds = logits.argmax(1)\n",
    "        miou_pc.update(preds, y)\n",
    "    vec = miou_pc.compute()  # shape [num_classes] on device\n",
    "    print(\"Per-class IoU:\", vec.detach().cpu().numpy())\n",
    "\n",
    "per_class_iou_gpu(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195025e",
   "metadata": {
    "id": "0195025e"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mask_to_color(mask_np):\n",
    "    out = np.zeros((*mask_np.shape,3), np.uint8)\n",
    "    for i,c in enumerate(CLASS_COLORS):\n",
    "        out[mask_np==i] = c\n",
    "    return out\n",
    "\n",
    "def overlay(img_bgr, mask_idx, alpha=0.45):\n",
    "    color = mask_to_color(mask_idx)\n",
    "    return cv2.addWeighted(img_bgr, 1-alpha, color, alpha, 0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_overlays(n=6):\n",
    "    model.eval()\n",
    "    try:\n",
    "        x,y = next(iter(val_dl))\n",
    "    except StopIteration:\n",
    "        print(\"No val batches to visualize.\"); return\n",
    "    o = model(x.to(device))['out'].argmax(1).cpu().numpy()\n",
    "    x_np = x.numpy()\n",
    "    mean = np.array([0.485,0.456,0.406]).reshape(1,3,1,1)\n",
    "    std  = np.array([0.229,0.224,0.225]).reshape(1,3,1,1)\n",
    "    x_np = np.clip(x_np*std + mean, 0, 1)\n",
    "    outdir = Path(cfg.OUT_DIR)/\"overlays_val\"; outdir.mkdir(parents=True, exist_ok=True)\n",
    "    k=min(n, x_np.shape[0])\n",
    "    for i in range(k):\n",
    "        img = (x_np[i].transpose(1,2,0)*255).astype(np.uint8)[:,:,::-1]\n",
    "        gt = y[i].numpy()\n",
    "        ov_pred = overlay(img, o[i])\n",
    "        ov_gt   = overlay(img, gt)\n",
    "        cv2.imwrite(str(outdir/f\"{i:02d}_pred.png\"), ov_pred)\n",
    "        cv2.imwrite(str(outdir/f\"{i:02d}_gt.png\"), ov_gt)\n",
    "    print(\"Saved overlays to\", outdir)\n",
    "\n",
    "print(\"▶️ Step: Overlay samples\")\n",
    "save_overlays(6)\n",
    "print(\"✅ Overlay saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33a862",
   "metadata": {
    "id": "7b33a862"
   },
   "source": [
    "# Task\n",
    "Perform Bayesian optimization using Optuna to find the optimal hyperparameters for the model, train the final model with the best hyperparameters, and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c4510",
   "metadata": {
    "id": "aa3c4510"
   },
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "### Subtask:\n",
    "Install `optuna` for Bayesian optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a359db",
   "metadata": {
    "id": "72a359db"
   },
   "source": [
    "**Reasoning**:\n",
    "The subtask is to install the `optuna` library. This can be done using pip in a code cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d2f1d",
   "metadata": {
    "id": "7c4d2f1d"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b17b5a",
   "metadata": {
    "id": "50b17b5a"
   },
   "source": [
    "## Define the objective function\n",
    "\n",
    "### Subtask:\n",
    "Create a function that trains and evaluates the model with a given set of hyperparameters and returns the validation metric to be optimized (e.g., mIoU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ece45",
   "metadata": {
    "id": "8f7ece45"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the objective function for Optuna, which takes a trial, suggests hyperparameters, trains the model for a few epochs, evaluates it, and returns the validation mIoU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005a66d",
   "metadata": {
    "id": "e005a66d"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [2, 4, 8])\n",
    "    epochs = 5 # Reduced number of epochs for faster optimization\n",
    "\n",
    "    # Update cfg with suggested hyperparameters\n",
    "    cfg.BATCH_SIZE = batch_size\n",
    "    cfg.EPOCHS = epochs\n",
    "    cfg.LR = lr\n",
    "\n",
    "    # Re-create dataloaders with new batch size\n",
    "    train_ds, train_dl = make_loader(TRAIN_IMG_DIR, TRAIN_MSK_DIR, \"train\", True, True)\n",
    "    val_ds,   val_dl   = make_loader(VAL_IMG_DIR,   VAL_MSK_DIR,   \"val\",   False, False)\n",
    "\n",
    "    # Instantiate model, optimizer, and scheduler\n",
    "    model = deeplabv3_resnet50(weights=None) # Start from scratch for each trial\n",
    "    try:\n",
    "        in_ch = model.classifier[-1].in_channels\n",
    "        model.classifier[-1] = nn.Conv2d(in_ch, cfg.NUM_CLASSES, 1)\n",
    "    except Exception:\n",
    "        in_ch = model.classifier[4].in_channels\n",
    "        model.classifier[4] = nn.Conv2d(in_ch, cfg.NUM_CLASSES, 1)\n",
    "\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.LR)\n",
    "    scheduler = OneCycleLR(\n",
    "        opt,\n",
    "        max_lr=cfg.LR,\n",
    "        epochs=cfg.EPOCHS,\n",
    "        steps_per_epoch=len(train_dl),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=10.0,\n",
    "        final_div_factor=10.0\n",
    "    )\n",
    "\n",
    "    # Define loss function\n",
    "    crit = ComboLoss(num_classes=cfg.NUM_CLASSES, ignore_index=255, ce_weight=0.85, dice_weight=0.15)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(1, cfg.EPOCHS + 1):\n",
    "        model.train()\n",
    "        for x, y in train_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            out_dict = model(x)\n",
    "            main_logits = out_dict['out']\n",
    "            aux_logits = out_dict.get('aux', None)\n",
    "\n",
    "            loss = crit(main_logits, y)\n",
    "            if aux_logits is not None:\n",
    "                loss = loss + 0.2 * nn.CrossEntropyLoss(ignore_index=255)(aux_logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    # Evaluate the model and calculate mIoU\n",
    "    miou = MulticlassJaccardIndex(num_classes=cfg.NUM_CLASSES, ignore_index=255).to(device)\n",
    "    miou.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            preds = model(x)['out'].argmax(1)\n",
    "            miou.update(preds, y)\n",
    "\n",
    "    val_miou = miou.compute().item()\n",
    "\n",
    "    return val_miou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceed848",
   "metadata": {
    "id": "eceed848"
   },
   "source": [
    "## Define the hyperparameter search space\n",
    "\n",
    "### Subtask:\n",
    "Specify the ranges or choices for the hyperparameters that Optuna should explore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861c355",
   "metadata": {
    "id": "f861c355"
   },
   "source": [
    "## Run the optimization study\n",
    "\n",
    "### Subtask:\n",
    "Execute the Optuna study to find the best hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d92eeb",
   "metadata": {
    "id": "80d92eeb"
   },
   "source": [
    "**Reasoning**:\n",
    "Import optuna, create a study, and run the optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d930f",
   "metadata": {
    "id": "fc7d930f"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c8315-f2d7-4d65-b546-a10ddd6e8ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908560f9-b29d-4080-aa8b-20ebc67d6227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b06d67-b6f5-41e7-8de1-b06f190996bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (floodnet-env)",
   "language": "python",
   "name": "floodnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1d8b6b48b5004c1e954adfac40e360ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37a1839c55ff423194b60477b09c840e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61d2c028babd42c79abb76609a091533",
       "IPY_MODEL_a45e090d017c4c4590b11b824600948a",
       "IPY_MODEL_ccb5cedd93cc4a0aad6030d6d55cba71"
      ],
      "layout": "IPY_MODEL_fa0daaf4810f4e61a3f769d0da747433"
     }
    },
    "3aa93ffc39f64d228ec7d2560f6e07f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41dccc7e7e7342aa9e2f074b204cc065": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61d2c028babd42c79abb76609a091533": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3943ed8c5a44048ac05d27631ab0f02",
      "placeholder": "​",
      "style": "IPY_MODEL_41dccc7e7e7342aa9e2f074b204cc065",
      "value": "Epoch 1 • Training:   1%"
     }
    },
    "7773863bf8f246198d4ee337bd97007f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a0cb55488d742a196e0c11c128e0676": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a45e090d017c4c4590b11b824600948a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7773863bf8f246198d4ee337bd97007f",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3aa93ffc39f64d228ec7d2560f6e07f1",
      "value": 5
     }
    },
    "ccb5cedd93cc4a0aad6030d6d55cba71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a0cb55488d742a196e0c11c128e0676",
      "placeholder": "​",
      "style": "IPY_MODEL_1d8b6b48b5004c1e954adfac40e360ad",
      "value": " 5/481 [00:14&lt;25:12,  3.18s/it]"
     }
    },
    "f3943ed8c5a44048ac05d27631ab0f02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa0daaf4810f4e61a3f769d0da747433": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
